{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H3PWdOaa7U9q",
    "outputId": "66f70415-6e33-4a55-e60b-b544cc85e93a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pvSpCiHV9jxn",
    "outputId": "0efc8730-3d88-4b7b-89d7-48af603d0a08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 58 directories and 0 images in 'fruit_dataset\\Train'.\n",
      "There are 0 directories and 1169 images in 'fruit_dataset\\Train\\ackee'.\n",
      "There are 0 directories and 1203 images in 'fruit_dataset\\Train\\apple'.\n",
      "There are 0 directories and 1126 images in 'fruit_dataset\\Train\\apricot'.\n",
      "There are 0 directories and 1169 images in 'fruit_dataset\\Train\\avocado'.\n",
      "There are 0 directories and 1162 images in 'fruit_dataset\\Train\\banana'.\n",
      "There are 0 directories and 1000 images in 'fruit_dataset\\Train\\bell pepper'.\n",
      "There are 0 directories and 1022 images in 'fruit_dataset\\Train\\betel nut'.\n",
      "There are 0 directories and 1077 images in 'fruit_dataset\\Train\\bitter gourd'.\n",
      "There are 0 directories and 927 images in 'fruit_dataset\\Train\\black cherry'.\n",
      "There are 0 directories and 1025 images in 'fruit_dataset\\Train\\black mullberry'.\n",
      "There are 0 directories and 1002 images in 'fruit_dataset\\Train\\blueberry'.\n",
      "There are 0 directories and 999 images in 'fruit_dataset\\Train\\bottle gourd'.\n",
      "There are 0 directories and 1062 images in 'fruit_dataset\\Train\\brazil nut'.\n",
      "There are 0 directories and 538 images in 'fruit_dataset\\Train\\burdekin plum'.\n",
      "There are 0 directories and 948 images in 'fruit_dataset\\Train\\caimito'.\n",
      "There are 0 directories and 1008 images in 'fruit_dataset\\Train\\cantaloupe'.\n",
      "There are 0 directories and 1007 images in 'fruit_dataset\\Train\\carambola'.\n",
      "There are 0 directories and 305 images in 'fruit_dataset\\Train\\cardon'.\n",
      "There are 0 directories and 1024 images in 'fruit_dataset\\Train\\cashew'.\n",
      "There are 0 directories and 1003 images in 'fruit_dataset\\Train\\cherry'.\n",
      "There are 0 directories and 1021 images in 'fruit_dataset\\Train\\chico'.\n",
      "There are 0 directories and 1007 images in 'fruit_dataset\\Train\\cocoa bean'.\n",
      "There are 0 directories and 1024 images in 'fruit_dataset\\Train\\coconut'.\n",
      "There are 0 directories and 1018 images in 'fruit_dataset\\Train\\corn kernel'.\n",
      "There are 0 directories and 1074 images in 'fruit_dataset\\Train\\custard apple'.\n",
      "There are 0 directories and 1002 images in 'fruit_dataset\\Train\\date'.\n",
      "There are 0 directories and 1035 images in 'fruit_dataset\\Train\\dragonfruit'.\n",
      "There are 0 directories and 830 images in 'fruit_dataset\\Train\\eggplant'.\n",
      "There are 0 directories and 810 images in 'fruit_dataset\\Train\\grape'.\n",
      "There are 0 directories and 1012 images in 'fruit_dataset\\Train\\grapefruit'.\n",
      "There are 0 directories and 740 images in 'fruit_dataset\\Train\\guava'.\n",
      "There are 0 directories and 740 images in 'fruit_dataset\\Train\\jackfruit'.\n",
      "There are 0 directories and 1018 images in 'fruit_dataset\\Train\\jambul'.\n",
      "There are 0 directories and 1010 images in 'fruit_dataset\\Train\\kiwi'.\n",
      "There are 0 directories and 1014 images in 'fruit_dataset\\Train\\lablab'.\n",
      "There are 0 directories and 1011 images in 'fruit_dataset\\Train\\lemon'.\n",
      "There are 0 directories and 1010 images in 'fruit_dataset\\Train\\lime'.\n",
      "There are 0 directories and 1008 images in 'fruit_dataset\\Train\\lychee'.\n",
      "There are 0 directories and 1003 images in 'fruit_dataset\\Train\\macadamia'.\n",
      "There are 0 directories and 1025 images in 'fruit_dataset\\Train\\mango'.\n",
      "There are 0 directories and 302 images in 'fruit_dataset\\Train\\midyim'.\n",
      "There are 0 directories and 1032 images in 'fruit_dataset\\Train\\olive'.\n",
      "There are 0 directories and 821 images in 'fruit_dataset\\Train\\orange'.\n",
      "There are 0 directories and 1027 images in 'fruit_dataset\\Train\\papaya'.\n",
      "There are 0 directories and 1405 images in 'fruit_dataset\\Train\\pea'.\n",
      "There are 0 directories and 864 images in 'fruit_dataset\\Train\\peanut'.\n",
      "There are 0 directories and 1007 images in 'fruit_dataset\\Train\\pear'.\n",
      "There are 0 directories and 928 images in 'fruit_dataset\\Train\\persimmon'.\n",
      "There are 0 directories and 1037 images in 'fruit_dataset\\Train\\pineapple'.\n",
      "There are 0 directories and 1013 images in 'fruit_dataset\\Train\\pomegranate'.\n",
      "There are 0 directories and 475 images in 'fruit_dataset\\Train\\pumpkin'.\n",
      "There are 0 directories and 1012 images in 'fruit_dataset\\Train\\raspberry'.\n",
      "There are 0 directories and 1009 images in 'fruit_dataset\\Train\\ridged gourd'.\n",
      "There are 0 directories and 1002 images in 'fruit_dataset\\Train\\strawberry'.\n",
      "There are 0 directories and 1000 images in 'fruit_dataset\\Train\\tomato'.\n",
      "There are 0 directories and 1005 images in 'fruit_dataset\\Train\\vanilla'.\n",
      "There are 0 directories and 996 images in 'fruit_dataset\\Train\\watermelon'.\n",
      "There are 0 directories and 1009 images in 'fruit_dataset\\Train\\zucchini'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "base_dir = 'fruit_dataset'\n",
    "train_dir = os.path.join(base_dir,  'Train')\n",
    "test_dir = os.path.join(base_dir, 'Test')\n",
    "for dirpath, dirnames, filenames in os.walk(train_dir):\n",
    "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zqeuHMkj9wnA",
    "outputId": "efaa77ac-22f8-4d74-fb0f-a49085cca59c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 56132 images belonging to 58 classes.\n",
      "Found 15668 images belonging to 58 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=30,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    target_size=(224, 224),\n",
    "                                                    batch_size=32,\n",
    "                                                    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(test_dir,\n",
    "                                                       target_size=(224, 224),\n",
    "                                                       batch_size=32,\n",
    "                                                       class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cuycbnIMLGt4",
    "outputId": "3de1dd0c-762d-4ef3-b16f-17c4153e7801"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load MobileNetV2 as base model\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base model to retain the pre-trained weights\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)  # Dense layer for more complex features\n",
    "predictions = Dense(58, activation='softmax')(x)  # Output layer for 58 classes\n",
    "\n",
    "# Final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The filepath provided must end in `.keras` (Keras model format). Received: filepath=model/fruit_model_epoch_{epoch:02d}.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelCheckpoint\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Save model after each epoch\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mModelCheckpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel/fruit_model_epoch_\u001b[39;49m\u001b[38;5;132;43;01m{epoch:02d}\u001b[39;49;00m\u001b[38;5;124;43m.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Path to save\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_weights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Save the entire model (architecture + weights)\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_best_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Save every epoch, not just the best one\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Monitor training loss (or validation loss/accuracy)\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\src\\callbacks\\model_checkpoint.py:191\u001b[0m, in \u001b[0;36mModelCheckpoint.__init__\u001b[1;34m(self, filepath, monitor, verbose, save_best_only, save_weights_only, mode, save_freq, initial_value_threshold)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 191\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    192\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe filepath provided must end in `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    193\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Keras model format). Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    194\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: The filepath provided must end in `.keras` (Keras model format). Received: filepath=model/fruit_model_epoch_{epoch:02d}.h5"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Save model after each epoch\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='model/fruit_model_epoch_{epoch:02d}.h5',  # Path to save\n",
    "    save_weights_only=False,  # Save the entire model (architecture + weights)\n",
    "    save_best_only=False,  # Save every epoch, not just the best one\n",
    "    monitor='loss',  # Monitor training loss (or validation loss/accuracy)\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "osqv6KSKNTNM",
    "outputId": "b1d2530d-dbe5-4be5-dcce-cea4c220b14f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6281 - loss: 1.3612"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Irfan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4198s\u001b[0m 2s/step - accuracy: 0.6282 - loss: 1.3610 - val_accuracy: 0.7792 - val_loss: 0.7511\n",
      "Epoch 2/10\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3889s\u001b[0m 2s/step - accuracy: 0.7615 - loss: 0.8031 - val_accuracy: 0.8111 - val_loss: 0.6352\n",
      "Epoch 3/10\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3862s\u001b[0m 2s/step - accuracy: 0.7911 - loss: 0.6973 - val_accuracy: 0.8184 - val_loss: 0.6100\n",
      "Epoch 4/10\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3853s\u001b[0m 2s/step - accuracy: 0.8151 - loss: 0.6156 - val_accuracy: 0.8101 - val_loss: 0.6387\n",
      "Epoch 5/10\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3918s\u001b[0m 2s/step - accuracy: 0.8273 - loss: 0.5790 - val_accuracy: 0.8568 - val_loss: 0.4721\n",
      "Epoch 6/10\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3908s\u001b[0m 2s/step - accuracy: 0.8387 - loss: 0.5446 - val_accuracy: 0.8557 - val_loss: 0.4907\n",
      "Epoch 7/10\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3901s\u001b[0m 2s/step - accuracy: 0.8478 - loss: 0.5052 - val_accuracy: 0.8625 - val_loss: 0.4568\n",
      "Epoch 8/10\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3686s\u001b[0m 2s/step - accuracy: 0.8533 - loss: 0.4859 - val_accuracy: 0.8605 - val_loss: 0.4654\n",
      "Epoch 9/10\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3670s\u001b[0m 2s/step - accuracy: 0.8614 - loss: 0.4602 - val_accuracy: 0.8777 - val_loss: 0.4138\n",
      "Epoch 10/10\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3720s\u001b[0m 2s/step - accuracy: 0.8618 - loss: 0.4479 - val_accuracy: 0.8602 - val_loss: 0.4755\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_generator, epochs=10, validation_data=validation_generator, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "bkC-2QkIjrBf",
    "outputId": "9e760ab4-a603-4ddb-97ba-716a450a43c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4131s\u001b[0m 2s/step - accuracy: 0.7974 - loss: 0.6930 - val_accuracy: 0.8391 - val_loss: 0.5822\n",
      "Epoch 2/5\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3104s\u001b[0m 2s/step - accuracy: 0.8580 - loss: 0.4673 - val_accuracy: 0.8618 - val_loss: 0.5204\n",
      "Epoch 3/5\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3525s\u001b[0m 2s/step - accuracy: 0.8760 - loss: 0.4063 - val_accuracy: 0.8821 - val_loss: 0.4196\n",
      "Epoch 4/5\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3357s\u001b[0m 2s/step - accuracy: 0.8886 - loss: 0.3583 - val_accuracy: 0.9039 - val_loss: 0.3361\n",
      "Epoch 5/5\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2604s\u001b[0m 1s/step - accuracy: 0.8946 - loss: 0.3408 - val_accuracy: 0.9075 - val_loss: 0.3286\n"
     ]
    }
   ],
   "source": [
    "# Unfreeze the last 10 layers of MobileNetV2 for fine-tuning\n",
    "for layer in base_model.layers[-10:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Re-compile and continue training\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history_finetune = model.fit(train_generator, epochs=5, validation_data=validation_generator, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "id": "kQDLbUFUjsQa",
    "outputId": "7f0a7e6c-11e7-4960-e023-5ba6cf2808e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9000 - loss: 0.3165\n",
      "Epoch 1: val_accuracy improved from -inf to 0.89763, saving model to models/best_model.keras\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3901s\u001b[0m 2s/step - accuracy: 0.9000 - loss: 0.3165 - val_accuracy: 0.8976 - val_loss: 0.3652\n",
      "Epoch 2/10\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9079 - loss: 0.2973\n",
      "Epoch 2: val_accuracy improved from 0.89763 to 0.90388, saving model to models/best_model.keras\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3354s\u001b[0m 2s/step - accuracy: 0.9079 - loss: 0.2973 - val_accuracy: 0.9039 - val_loss: 0.3459\n",
      "Epoch 3/10\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9124 - loss: 0.2834\n",
      "Epoch 3: val_accuracy improved from 0.90388 to 0.91199, saving model to models/best_model.keras\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3807s\u001b[0m 2s/step - accuracy: 0.9124 - loss: 0.2834 - val_accuracy: 0.9120 - val_loss: 0.3290\n",
      "Epoch 4/10\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9158 - loss: 0.2641\n",
      "Epoch 4: val_accuracy did not improve from 0.91199\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3501s\u001b[0m 2s/step - accuracy: 0.9158 - loss: 0.2641 - val_accuracy: 0.8972 - val_loss: 0.3780\n",
      "Epoch 5/10\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9156 - loss: 0.2660\n",
      "Epoch 5: val_accuracy improved from 0.91199 to 0.92801, saving model to models/best_model.keras\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3368s\u001b[0m 2s/step - accuracy: 0.9156 - loss: 0.2660 - val_accuracy: 0.9280 - val_loss: 0.2700\n",
      "Epoch 6/10\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9212 - loss: 0.2456\n",
      "Epoch 6: val_accuracy did not improve from 0.92801\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3397s\u001b[0m 2s/step - accuracy: 0.9212 - loss: 0.2456 - val_accuracy: 0.9267 - val_loss: 0.2715\n",
      "Epoch 7/10\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9270 - loss: 0.2310\n",
      "Epoch 7: val_accuracy did not improve from 0.92801\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3454s\u001b[0m 2s/step - accuracy: 0.9270 - loss: 0.2310 - val_accuracy: 0.9265 - val_loss: 0.2546\n",
      "Epoch 8/10\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9311 - loss: 0.2119\n",
      "Epoch 8: val_accuracy improved from 0.92801 to 0.92928, saving model to models/best_model.keras\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3856s\u001b[0m 2s/step - accuracy: 0.9311 - loss: 0.2119 - val_accuracy: 0.9293 - val_loss: 0.2616\n",
      "Epoch 9/10\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9303 - loss: 0.2151\n",
      "Epoch 9: val_accuracy did not improve from 0.92928\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3485s\u001b[0m 2s/step - accuracy: 0.9303 - loss: 0.2151 - val_accuracy: 0.9253 - val_loss: 0.2737\n",
      "Epoch 10/10\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9313 - loss: 0.2091\n",
      "Epoch 10: val_accuracy improved from 0.92928 to 0.93081, saving model to models/best_model.keras\n",
      "\u001b[1m1755/1755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3246s\u001b[0m 2s/step - accuracy: 0.9313 - loss: 0.2091 - val_accuracy: 0.9308 - val_loss: 0.2517\n",
      "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m426s\u001b[0m 868ms/step - accuracy: 0.9320 - loss: 0.2489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.25174808502197266\n",
      "Validation accuracy: 0.9308143854141235\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import os\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Early stopping to avoid overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Checkpoint to save the best model\n",
    "checkpoint = ModelCheckpoint(filepath='models/best_model.keras', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "\n",
    "# Train the model with callbacks\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[early_stopping, checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(validation_generator)\n",
    "print('Validation loss:', score[0])\n",
    "print('Validation accuracy:', score[1])\n",
    "\n",
    "# Save the final model\n",
    "model.save('models/fruit_classifier_model.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000016B2864ACA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000016B2864ACA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "Predicted class: papaya\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load your saved model\n",
    "model = load_model('models/best_model.keras')\n",
    "\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_path = 'papaya.jpg'  # Replace with the path to your test image\n",
    "img = load_img(image_path, target_size=(224, 224))  # Resize to the target input size\n",
    "img_array = img_to_array(img) / 255.0  # Convert image to array and normalize pixel values\n",
    "img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "\n",
    "# Make prediction\n",
    "predictions = model.predict(img_array)\n",
    "\n",
    "# Get the predicted class index\n",
    "predicted_class_index = np.argmax(predictions[0])\n",
    "\n",
    "# Get class labels (assuming you have used a data generator like ImageDataGenerator)\n",
    "class_labels = {v: k for k, v in train_generator.class_indices.items()}  # Reverse the mapping\n",
    "\n",
    "# Get the predicted label\n",
    "predicted_class = class_labels[predicted_class_index]\n",
    "print(f\"Predicted class: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94GaBW1Wj-yA"
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(validation_generator)\n",
    "print('Validation loss:', score[0])\n",
    "print('Validation accuracy:', score[1])\n",
    "\n",
    "model.save('fruit_classifier_model.h5')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
